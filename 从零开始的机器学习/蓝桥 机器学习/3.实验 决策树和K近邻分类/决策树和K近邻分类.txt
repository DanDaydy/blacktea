机器学习经典、通用的定义：

假设用 P 来评估计算机程序在某任务类 T 上的性能，若一个程序利用经验 E 在任务 T 上获得了性能改善，则我们就说关于 T 和 P, 该程序对 E 进行了学习。

在不同的问题设定下，T、P、E 可能指完全不同的东西。机器学习中一些流行的任务 T 包括：
Markdown Code

    分类：基于特征将实例分为某一类。
    回归：基于实例的其他特征预测该实例的数值型目标特征。
    聚类：基于实例的特征实现实例的分组，从而让组内成员比组间成员更为相似。
    异常检测：寻找与其他样本或组内实例有很大区别的实例。
    其他更多任务

经验 E 指的是数据（没有数据我们什么也干不了）。根据训练方式，机器学习算法可以分为监督（supervised）和无监督（unsupervised）两类。无监督学习需要训练含有很多特征的数据集，然后学习出这个数据集上有用的结构性质。而监督学习的数据集除了含有很多特征外，它的每个样本都要有一个标签（label）或目标（target）。


示例

分类和回归属于监督学习问题。例如，作为信贷机构，我们可能希望根据客户累积的数据预测贷款违约情况。在这里，经验 E 是已有的训练数据，即实例（客户）的集合，一组特征（例如年龄、薪水、贷款类型、以往违约记录等），一个目标变量（他们是否会违约）。

由于需要预测的目标变量是「他们是否会违约」，所以这是一个二元分类问题。

如果你转而预测贷款会超期多久，那么需要预测的目标变量变成了一个连续值（时间），这就成为一个回归问题了。

最后，关于算法表现的评估度量 P。不同问题和算法的度量不同，当学习新算法时，我们将讨论这一点。就目前而言，本次实验将使用分类算法中的一个简单度量标准，即准确率（Accuracy）。

下面看看分类和回归这两个监督学习问题。

=========================

决策树

决策树是分类与回归问题中常用的方法之一。其实不仅是机器学习领域，在每天的日常决策中，我们都在使用决策树。流程图实际上就是决策树的可视化表示


如何构建决策树


熵

熵是一个在物理、信息论和其他领域中广泛应用的重要概念，可以衡量获得的信息量。对于具有 N 种可能状态的系统而言，熵的定义如下：

S = -\sum_{i=1}^N p_i log_2 p_i

其中，𝑝𝑖 是系统位于第 i 个状态的概率
熵可以描述为系统的混沌程度，熵越高，系统的有序性越差，反之亦然
熵将帮助我们高效的分割数据


玩具示例

熵的下降被称为信息增益（IG）

IG(Q) =  S_O - \sum_{i=1}^{p}\frac{N_i}{N}S_i



决策树构建算法
构建决策树的流行算法（如 ID3 或 C4.5）的核心，是贪婪最大化信息增益：

在每一步，算法都会选择能在分割后给出最大信息增益的变量。

接着递归重复这一流程，直到熵为零（或者，为了避免过拟合，直到熵为某个较小的值）。

不同的算法使用不同的推断，通过「提前停止」或「截断」以避免构建出过拟合的树。


===============================

分类问题中其他的分割质量标准

上面我们讨论了熵是如何衡量树的分区的，但还有其他指标来衡量分割的好坏：

基尼不确定性（Gini uncertainty）

错分率（Misclassification error）

实践中几乎从不使用错分率，而基尼不确定性和信息增益的效果差不多。

熵的图像和两倍的基尼不确定性图像非常接近。因此，在实践中，这两个指标的效果基本上是一样的。

------

示例

下面用一棵决策树拟合一些合成数据。这些合成数据属于两个不同的类别，这两个类别的均值不同，但都呈现正态分布。





================

我们如何「读懂」这颗决策树？

上个示例中，总共有 200 个合成数据（样本），每个分类各有 100 个合成数据。

初始状态的熵是最大的，即 𝑆=1。

接着，通过比较 𝑥2 与 1.211 的大小进行第一次分割，将样本分成两组（你可以在上图中找到这一部分边界）。

基于这一次分割，左右两组的熵都下降了。

这一过程持续进行，直到树的深度达到 3。

在上图中，属于第一类的样本数量越多，该节点的橙色就越深，属于第二类的样本越多，该节点的蓝色就越深。
若两类样本的数量相等，则为白色，比如根节点的两类样本数量相同，所以它是白色的


==================

决策树如何应用到数值特征？

假设有一个数值特征「年龄」，该特征有大量的唯一值。决策树将通过查看「年龄 < 17」、「年龄 < 22.87」这样的二元属性寻找最好的分割，分割的好坏由某种信息增益标准衡量。但在构建树的每一步中，会有过多的二元属性可供选择，比如「薪水」同样能以很多方式进行分割，为了解决这一问题，

我们经常使用启发式算法来限制选择的属性数量。


决策树处理数值特征最简单的启发式算法是升序排列它的值，然后只关注目标变量发生改变的那些值


此外，当数据集具有大量数值特征，且每个特征具有大量唯一值时，只选择最高的N个阈值，即，仅仅使用提供最高增益的前N个值。这一过程可以看成是构造了一棵深度为 1 的树，计算熵（或基尼不确定性），然后选择最佳阈值用于比较。比方说，如果我们根据「薪水 ≤ 34.5」分割，左子组的熵为 0（所有客户都是「不好的」），而右边的熵为 0.954（3 个「不好的」，5 个「好的」，你可以自行确认这一点，这将作为作业的一部分），信息增益大概是 0.3。如果我们根据「薪水 ≤ 95」分割，左边的子组的熵会是 0.97（6 个「不好的」，4 个「好的」），而右边的熵会是 0（该组只包含 1 个对象），信息增益大约是 0.11。如果以这样的方式计算每种分区的信息增益，那么在使用所有特征构造一棵大决策树之前就可以选出每个数值特征的阈值。




=========================


树的关键参数

我们可以构建一个决策树，直到每个叶节点只有一个实例，但这样做容易过拟合，导致其在新数据上的表现不佳


但在两种情况下，树可以被构建到最大深度（每个叶节点只有一个实例）：

    随机森林。它将构建为最大深度的单个树的响应进行平均（稍后我们将讨论为什么要这样做）

    决策树修剪。在这种方法中，树首先被构造成最大深度。然后，从底部开始，基于交叉验证来比较有分区/无分区情形下树的质量情况，进而移除树的一些节点


常见的解决决策树过拟合的方法为：

    人工限制深度或叶节点的最少样本数。

    对树进行剪枝。


-----------------------------------------------------

scikit-learn 的 DecisionTreeClassifier 类

sklearn.tree.DecisionTreeClassifier 类的主要参数为：

    max_depth 树的最大深度；
    max_features 搜索最佳分区时的最大特征数（特征很多时，设置这个参数很有必要，因为基于所有特征搜索分区会很「昂贵」）；
    min_samples_leaf 叶节点的最少样本数。

树的参数需要根据输入数据设定，通常通过交叉验证可以确定参数范围



---------------


回归问题中的决策树

D = \frac{1}{\ell} \sum_{i=1}^{\ell}(y_i - \frac{1}{\ell}\sum_{j=1}^{\ell}y_j)^2


===================================================

最近邻方法


最近邻方法（K 近邻或 k-NN）是另一个非常流行的分类方法。当然，也可以用于回归问题。和决策树类似，这是最容易理解的分类方法之一。这一方法遵循紧密性假说：如果样本间的距离能以足够好的方法衡量，那么相似的样本更可能属于同一分类。


在最近邻方法中，为了对测试集中的每个样本进行分类，需要依次进行以下操作：

    计算训练集中每个样本之间的距离。
    从训练集中选取 k 个距离最近的样本。
    测试样本的类别将是它 k 个最近邻中最常见的分类。


这一方式的显著特点是它具有惰性：当需要对测试样本进行分类时，计算只在预测阶段进行。由于这种特点，最近邻方法事先并不基于训练样本创建模型，这与上文提到的决策树不同。决策树是基于训练集构建的，在预测阶段仅通过遍历决策树就可以实现快速地分类。


最近邻方法的实际应用

    在某些案例中，k-NN 可以作为一个模型的基线。
    在 Kaggle 竞赛中，k-NN 常常用于构建元特征（即 k-NN 的预测结果作为其他模型的输入），或用于堆叠/混合。
    最近邻方法还可以扩展到推荐系统等任务中。
    在大型数据集上，常常使用逼近方法搜索最近邻。


k-NN 分类/回归的效果取决于一些参数：

    邻居数 k。
    样本之间的距离度量（常见的包括 Hamming，欧几里得，余弦和 Minkowski 距离）。注意，大部分距离要求数据在同一尺度下，例如「薪水」特征的数值在千级，「年龄」特征的数值却在百级，如果直接将他们丢进最近邻模型中，「年龄」特征就会受到比较大的影响。
    邻居的权重（每个邻居可能贡献不同的权重，例如，样本越远，权重越低）。



scikit-learn 的 KNeighborsClassifier 类

sklearn.neighbors.KNeighborsClassifier 类的主要参数为：

    weights：可设为 uniform（所有权重相等），distance（权重和到测试样本的距离成反比），或任何其他用户自定义的函数。
    algorithm（可选）：可设为 brute、ball_tree、KD_tree、auto。若设为 brute，通过训练集上的网格搜索来计算每个测试样本的最近邻；若设为 ball_tree 或 KD_tree，样本间的距离储存在树中，以加速寻找最近邻；若设为 auto，将基于训练集自动选择合适的寻找最近邻的方法。
    leaf_size（可选）：若寻找最近邻的算法是 BallTree 或 KDTree，则切换为网格搜索所用的阈值。
    metric：可设为 minkowski、manhattan、euclidean、chebyshev 或其他。


选择模型参数和交叉验证

机器学习算法的主要任务是可以「泛化」未曾见过的数据。由于我们无法立刻得知模型在新数据上的表现（因为还不知道目标变量的真值），因此有必要牺牲一小部分数据，来验证模型的质量，即将一小部分数据作为留置集。


通常采用下述两种方法之一来验证模型的质量：

    留置法。保留一小部分数据（一般是 20% 到 40%）作为留置集，在其余数据上训练模型（原数据集的 60%-80%），然后在留置集上验证模型的质量。
    交叉验证。最常见的情形是 k 折交叉验证，如下图所示。


在 k 折交叉验证中，模型在原数据集的 𝐾−1 个子集上进行训练（上图白色部分），然后在剩下的 1 个子集上验证表现，重复训练和验证的过程，每次使用不同的子集（上图橙色部分），总共进行 K 次，由此得到 K 个模型质量评估指数，通常用这些评估指数的求和平均数来衡量分类/回归模型的总体质量。

交叉验证是机器学习中非常重要的技术，同时也应用于统计学和经济学领域。它有助于我们进行超参数调优、模型比较、特征评估等其他重要操作。



-------------

应用样例

在客户离网率预测任务中使用决策树和最近邻方法




决策树的复杂情况



在 MNIST 手写数字识别任务中应用决策树和 k-NN


最近邻方法的复杂情形





==============================

决策树和最近邻方法的优势和劣势

优势：

    生成容易理解的分类规则，这一属性称为模型的可解释性。例如它生成的规则可能是「如果年龄不满 25 岁，并对摩托车感兴趣，那么就拒绝发放贷款」。
    很容易可视化，即模型本身（树）和特定测试对象的预测（穿过树的路径）可以「被解释」。
    训练和预测的速度快。
    较少的参数数目。
    支持数值和类别特征。

劣势：

    决策树对输入数据中的噪声非常敏感，这削弱了模型的可解释性。
    决策树构建的边界有其局限性：它由垂直于其中一个坐标轴的超平面组成，在实践中比其他方法的效果要差。
    我们需要通过剪枝、设定叶节点的最小样本数、设定树的最大深度等方法避免过拟合。
    不稳定性，数据的细微变动都会显著改变决策树。这一问题可通过决策树集成方法来处理（以后的实验会介绍）。
    搜索最佳决策树是一个「NP 完全」（NP-Complete）问题。了解什么是 NP-Complete 请点击 这里。实践中使用的一些推断方法，比如基于最大信息增益进行贪婪搜索，并不能保证找到全局最优决策树。
    倘若数据中出现缺失值，将难以创建决策树模型。Friedman 的 CART 算法中大约 50% 的代码是为了处理数据中的缺失值（现在 sklearn 实现了这一算法的改进版本）。
    这一模型只能内插，不能外推（随机森林和树提升方法也是如此）。也就是说，倘若你预测的对象在训练集所设置的特征空间之外，那么决策树就只能做出常数预测。比如，在我们的黄球和蓝球的例子中，这意味着模型将对所有位于 >19 或 <0 的球做出同样的预测。






最近邻方法

优势：

    实现简单。
    研究很充分。
    通常而言，在分类、回归、推荐问题中第一个值得尝试的方法就是最近邻方法。
    通过选择恰当的衡量标准或核，它可以适应某一特定问题。

劣势：

    和其他复合算法相比，这一方法速度较快。但是，现实生活中，用于分类的邻居数目通常较大（100-150），在这一情形下，k-NN 不如决策树快。
    如果数据集有很多变量，很难找到合适的权重，也很难判定哪些特征对分类/回归不重要。
    依赖于对象之间的距离度量，默认选项欧几里得距离常常是不合理的。你可以通过网格搜索参数得到良好的解，但在大型数据集上的耗时很长。
    没有理论来指导我们如何选择邻居数，故而只能进行网格搜索（尽管基本上所有的模型，在对其超参数进行调整时都使用网格搜索的方法）。在邻居数较小的情形下，该方法对离散值很敏感，也就是说，有过拟合的倾向。
    由于「维度的诅咒」，当数据集存在很多特征时它的表现不佳。









